<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jesse Krijthe</title>
    <link>http://www.jessekrijthe.com/categories/machine-learning/</link>
    <description>Recent content on Jesse Krijthe</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>jkrijthe@gmail.com (Jesse Krijthe)</managingEditor>
    <webMaster>jkrijthe@gmail.com (Jesse Krijthe)</webMaster>
    <lastBuildDate>Tue, 16 Jan 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/machine-learning/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>PhD Defense</title>
      <link>http://www.jessekrijthe.com/articles/phd-defense/</link>
      <pubDate>Tue, 16 Jan 2018 00:00:00 +0000</pubDate>
      <author>jkrijthe@gmail.com (Jesse Krijthe)</author>
      <guid>http://www.jessekrijthe.com/articles/phd-defense/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://www.jessekrijthe.com/img/thesis-stack.jpg&#34; width=&#34;400&#34; class=&#34;sidenote&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I defended my PhD thesis! The thesis &amp;ldquo;Robust Semi-supervised Learning: Projections, Limits &amp;amp; Constraints&amp;rdquo; is about exploring the limits of the guarantees one can give for whether a semi-supervised learner will outperform its supervised counterpart. In other words: the limits of the usefulness of additional unlabeled data in a supervised learning setting.&lt;/p&gt;

&lt;p&gt;While it seems to make sense you would want such guarantees before using semi-supervised methods to incorporate unlabeled data into the learning process, we show that without additional assumptions, for many supervised learners, semi-supervised alternatives with strict non-degradation guarantees can not be constructed. Perhaps surprisingly, however, for some supervised learners/loss functions, semi-supervised methods can be constructed that give strict non-degradation guarantees. For details of how these concepts (guarantees, performance) were defined, please see the thesis itself. After thinking about this topic for such a long time, I&amp;rsquo;m quite proud of how the thesis turned out. You can find it &lt;a href=&#34;http://www.jessekrijthe.com/thesis.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(As an aside: the thesis is typeset using R&amp;rsquo;s knitr package and Latex. You can find the source code to generate figures and text &lt;a href=&#34;https://github.com/jkrijthe/RobustSSL&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;As part of the thesis defense, at most Dutch universities, you are expected to include a set of propositions, some pertaining to the core claims made in the thesis, some about your field of research and some about science/society in general. These were my propositions included in the thesis:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Semi-supervised learning without additional assumptions, beyond those inherent in the supervised classifier, is possible.&lt;/li&gt;
&lt;li&gt;One can guarantee performance improvement of some semi-supervised learners over their supervised counterparts.&lt;/li&gt;
&lt;li&gt;Truly safe semi-supervised learning is impossible for a large class of commonly used classifiers.&lt;/li&gt;
&lt;li&gt;Considering a classification method&amp;rsquo;s performance in terms of the actual loss it minimizes at train time gives useful insights.&lt;/li&gt;
&lt;li&gt;There is a limit to the usefulness of asymptotic results.&lt;/li&gt;
&lt;li&gt;Rather than hoping for practice to better correspond to current statistical methods, we need new methods that better match the adaptive way statistics is used in practice.&lt;/li&gt;
&lt;li&gt;The focus in statistical practice on hypothesis testing is feeding society&amp;rsquo;s appetite to clear cut answers in a reality where none are available.&lt;/li&gt;
&lt;li&gt;Data is uninteresting without a model, while a model can be interesting without data.&lt;/li&gt;
&lt;li&gt;Publishers have become a dispensible part of scientific communication.&lt;/li&gt;
&lt;li&gt;Our unwillingness or inability to define our actual goals, combined with a need for certainty, lead to surrogate measures (e.g. GDP, H-index, wealth, `likes&amp;rsquo; on social media) that are actively harmful.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I again want to thank my opposition committee: Ludmila Kuncheva, Peter Grunwald, Jelle Goeman, Erik van den Akker and Tom Heskes for the insightful questions that made for a super enjoyable discussion.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Favourite Work at ICML 2015</title>
      <link>http://www.jessekrijthe.com/articles/icml2015/</link>
      <pubDate>Wed, 05 Aug 2015 13:09:13 +0000</pubDate>
      <author>jkrijthe@gmail.com (Jesse Krijthe)</author>
      <guid>http://www.jessekrijthe.com/articles/icml2015/</guid>
      <description>&lt;p&gt;This post is just to remind myself of some of my favourite posters/presentations that I saw while attending ICML. I have undoubtably missed a lot of interesting stuff. If you have any particular suggestions, please let me know!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jmlr.org/proceedings/papers/v37/betancourt15.pdf&#34;&gt;The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling&lt;/a&gt;&lt;br/&gt;
&lt;em&gt;Michael Betancourt&lt;/em&gt;&lt;br/&gt;
I liked the topic and the kind of analysis and I especially liked his clear style of presentation. Moreover, there was quite a lively discussion about whether this incompatibility is actually a problem, or whether it focussed too much on only the bias that is introduced by naive subsampling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jmlr.org/proceedings/papers/v37/salimans15.pdf&#34;&gt;Markov Chain Monte Carlo and Variational Inference: Bridging the Gap&lt;/a&gt;&lt;br/&gt;
&lt;em&gt;Tim Salimans, Diederik Kingma, Max Welling&lt;/em&gt;&lt;br/&gt;
The presentation and poster were a bit hard for me to follow but the problem seems important.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jmlr.org/proceedings/papers/v37/lopez-paz15.pdf&#34;&gt;Towards a Learning Theory of Cause-Effect Inference&lt;/a&gt;&lt;br/&gt;
&lt;em&gt;David Lopez-Paz, Krikamol Muandet, Bernhard Sch√∂lkopf, Iliya Tolstikhin&lt;/em&gt;&lt;br/&gt;
Interesting use of Maximum Mean Discrepancy in a clear analysis of an important problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jmlr.org/proceedings/papers/v37/blundell15.pdf&#34;&gt;Weight Uncertainty in Neural Network&lt;/a&gt;&lt;br/&gt;
&lt;em&gt;Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra&lt;/em&gt;&lt;br/&gt;
I have not looked into how exactly their approach is different from previous attempts at incorporating weight uncertainty, but the updates for the weight parameters seemed surprisingly simple.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jmlr.org/proceedings/papers/v37/ramaswamy15.pdf&#34;&gt;Convex Calibrated Surrogates for Hierarchical Classification&lt;/a&gt;&lt;br/&gt;
&lt;em&gt;Harish Ramaswamy, Ambuj Tewari, Shivani Agarwal&lt;/em&gt;&lt;br/&gt;
I like this idea of classification calibrated losses and this seems like an interesting extension to hierarchical loss functions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jmlr.org/proceedings/papers/v37/narasimhana15.pdf&#34;&gt;Optimizing Non-decomposable Performance Measures: A Tale of Two Classes&lt;/a&gt;&lt;br/&gt;
&lt;em&gt;Harikrishna Narasimhan, Purushottam Kar, Prateek Jain&lt;/em&gt;&lt;br/&gt;
The authors consider functions of the true positive rate and true negative rate and come up with two classes of such functions and an approach to maximize them. The one class includes measures like the G-mean and the H-mean, while the other class includes the F-measure and Jaccard coefficient.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jmlr.org/proceedings/papers/v37/jiao15.pdf&#34;&gt;The Kendall and Mallows Kernels for Permutations&lt;/a&gt;&lt;br/&gt;
&lt;em&gt;Yunlong Jiao &amp;amp; Jean-Philippe Vert&lt;/em&gt;&lt;br/&gt;
The authors consider the problem of learning from permutations or rankings instead of vector of real valued numbers. In particular, they construct PSD kernels based on Kendall&amp;rsquo;s coefficient and Mallows kernel in order to apply kernel methods to the problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://arxiv.org/pdf/1501.05427v3&#34;&gt;Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)&lt;/a&gt;&lt;br/&gt;
&lt;em&gt;Maurizio Filippone &amp;amp; Raphael Engler&lt;/em&gt;&lt;br/&gt;
This seems to tackle the important problem exact quantification of uncertainty in covariance parameters for gaussian processes with seemingly few constraints on the number type of covariance function.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://jmlr.org/proceedings/papers/v37/hugginsb15.pdf&#34;&gt;Risk and Regret of Hierarchical Bayesian Learners&lt;/a&gt;&lt;br/&gt;
&lt;em&gt;Jonathan H. Huggins &amp;amp; Joshua B. Tenenbaum&lt;/em&gt;&lt;br/&gt;
Again, an interesting analysis of an important problem, although it will take me some more time to study the actual result.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>